{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bb2267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736baa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# def temporal_anomaly_scoring(\n",
    "#     monitoring_cube,\n",
    "#     BaselineModel,\n",
    "#     features=(\"NDVI\", \"NBR\", \"BSI\", \"B11\", \"B12\"),\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     STEP 3 — Temporal Anomaly Scoring (Inference)\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     monitoring_cube : xarray.Dataset\n",
    "#         Dimensions: (time, y, x)\n",
    "#         Variables : spectral indices / bands\n",
    "\n",
    "#     BaselineModel : dict\n",
    "#         {\n",
    "#             \"scaler\": fitted StandardScaler,\n",
    "#             \"pca\": fitted PCA\n",
    "#         }\n",
    "\n",
    "#     features : list or tuple\n",
    "#         Feature names in correct order (MUST match baseline)\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     outputs : dict\n",
    "#         {\n",
    "#             \"monitor_df\"    : DataFrame with anomaly_score,\n",
    "#             \"anomaly_maps\" : dict[datetime -> xarray.DataArray],\n",
    "#             \"stats\"        : dict (summary statistics)\n",
    "#         }\n",
    "#     \"\"\"\n",
    "\n",
    "#     # ─────────────────────────────────────────────\n",
    "#     # 1. Flatten monitoring cube\n",
    "#     # ─────────────────────────────────────────────\n",
    "#     monitor_df = (\n",
    "#         monitoring_cube\n",
    "#         .to_dataframe()\n",
    "#         .reset_index()\n",
    "#         .dropna()\n",
    "#     )\n",
    "\n",
    "#     # ─────────────────────────────────────────────\n",
    "#     # 2. Load baseline model\n",
    "#     # ─────────────────────────────────────────────\n",
    "#     scaler = BaselineModel[\"scaler\"]\n",
    "#     pca    = BaselineModel[\"pca\"]\n",
    "\n",
    "#     # ─────────────────────────────────────────────\n",
    "#     # 3. Normalize → project → reconstruct\n",
    "#     # ─────────────────────────────────────────────\n",
    "#     X_mon = scaler.transform(monitor_df[list(features)])\n",
    "#     X_latent = pca.transform(X_mon)\n",
    "#     X_recon = pca.inverse_transform(X_latent)\n",
    "\n",
    "#     # ─────────────────────────────────────────────\n",
    "#     # 4. Compute anomaly score\n",
    "#     # ─────────────────────────────────────────────\n",
    "#     monitor_df[\"anomaly_score\"] = np.mean(\n",
    "#         (X_mon - X_recon) ** 2,\n",
    "#         axis=1\n",
    "#     )\n",
    "\n",
    "#     # ─────────────────────────────────────────────\n",
    "#     # 5. Rebuild anomaly rasters (per date)\n",
    "#     # ─────────────────────────────────────────────\n",
    "#     anomaly_maps = {}\n",
    "\n",
    "#     for t in monitor_df[\"time\"].unique():\n",
    "#         df_t = monitor_df[monitor_df[\"time\"] == t]\n",
    "\n",
    "#         anomaly_maps[pd.to_datetime(t)] = (\n",
    "#             df_t\n",
    "#             .set_index([\"y\", \"x\"])[\"anomaly_score\"]\n",
    "#             .to_xarray()\n",
    "#         )\n",
    "\n",
    "#     # ─────────────────────────────────────────────\n",
    "#     # 6. Summary statistics (VERY IMPORTANT later)\n",
    "#     # ─────────────────────────────────────────────\n",
    "#     stats = {\n",
    "#         \"mean\": float(monitor_df[\"anomaly_score\"].mean()),\n",
    "#         \"std\": float(monitor_df[\"anomaly_score\"].std()),\n",
    "#         \"p90\": float(monitor_df[\"anomaly_score\"].quantile(0.90)),\n",
    "#         \"p95\": float(monitor_df[\"anomaly_score\"].quantile(0.95)),\n",
    "#         \"p99\": float(monitor_df[\"anomaly_score\"].quantile(0.99)),\n",
    "#         \"min\": float(monitor_df[\"anomaly_score\"].min()),\n",
    "#         \"max\": float(monitor_df[\"anomaly_score\"].max()),\n",
    "#         \"n_pixels\": int(len(monitor_df))\n",
    "#     }\n",
    "\n",
    "#     return monitor_df, anomaly_maps, stats\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92e83950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporial_anomaly_pipeline_fixed.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def temporal_anomaly_scoring_safe(\n",
    "    monitoringcube,\n",
    "    BaselineModel,\n",
    "    features=(\"NDVI\", \"NBR\", \"BSI\", \"B11\", \"B12\"),\n",
    "):\n",
    "    \"\"\"\n",
    "    Safer temporal anomaly scoring:\n",
    "    - Does not drop all NaNs globally.\n",
    "    - Computes scores per-date only for valid pixels, then merges back.\n",
    "    \"\"\"\n",
    "    scaler = BaselineModel[\"scaler\"]\n",
    "    pca = BaselineModel[\"pca\"]\n",
    "    feats = list(features)\n",
    "\n",
    "    # Convert cube to long DataFrame with possible NaNs\n",
    "    df_all = monitoringcube[feats].to_dataframe().reset_index()\n",
    "\n",
    "    # Prepare output column\n",
    "    df_all[\"anomalyscore\"] = np.nan\n",
    "\n",
    "    # Compute anomaly per time slice on valid rows\n",
    "    for t, dft in df_all.groupby(\"time\"):\n",
    "        mask_valid = dft[feats].notna().all(axis=1)\n",
    "        if not mask_valid.any():\n",
    "            continue\n",
    "\n",
    "        Xmon = dft.loc[mask_valid, feats].values\n",
    "        Xscaled = scaler.transform(Xmon)\n",
    "        Xlatent = pca.transform(Xscaled)\n",
    "        Xrecon = pca.inverse_transform(Xlatent)\n",
    "        recon_err = np.mean((Xscaled - Xrecon) ** 2, axis=1)\n",
    "\n",
    "        df_all.loc[dft.index[mask_valid], \"anomalyscore\"] = recon_err\n",
    "\n",
    "    # Build xarray anomaly maps\n",
    "    anomalymaps = {}\n",
    "    for t, dft in df_all.groupby(\"time\"):\n",
    "        anomalymaps[pd.to_datetime(t)] = (\n",
    "            dft.set_index([\"y\", \"x\"])[\"anomalyscore\"]\n",
    "            .to_xarray()\n",
    "            .rename(\"anomalyscore\")\n",
    "        )\n",
    "\n",
    "    stats = {\n",
    "        \"mean\": float(df_all[\"anomalyscore\"].mean(skipna=True)),\n",
    "        \"std\": float(df_all[\"anomalyscore\"].std(skipna=True)),\n",
    "        \"p90\": float(df_all[\"anomalyscore\"].quantile(0.90)),\n",
    "        \"p95\": float(df_all[\"anomalyscore\"].quantile(0.95)),\n",
    "        \"p99\": float(df_all[\"anomalyscore\"].quantile(0.99)),\n",
    "        \"min\": float(df_all[\"anomalyscore\"].min(skipna=True)),\n",
    "        \"max\": float(df_all[\"anomalyscore\"].max(skipna=True)),\n",
    "        \"npixels\": int(df_all[\"anomalyscore\"].notna().sum()),\n",
    "    }\n",
    "\n",
    "    return df_all, anomalymaps, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ffa9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_df, anomaly_maps, stats  = temporal_anomaly_scoring(\n",
    "    monitoring_cube=monitoring_cube,\n",
    "    BaselineModel=BaselineModel\n",
    ")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
