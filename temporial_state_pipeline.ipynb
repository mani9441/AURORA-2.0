{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cdc9baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1808d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_excavation_pipeline(\n",
    "    monitordf,\n",
    "    window=3,\n",
    "    anomaly_quantile=0.90,\n",
    "    persistence_days_rise=30,\n",
    "    persistence_days_fall=60,\n",
    "    min_persist_rise=2,\n",
    "    min_persist_fall=2,\n",
    "    max_gap_days_for_delta=30,\n",
    "):\n",
    "    \"\"\"\n",
    "    Improved temporal excavation detection pipeline.\n",
    "\n",
    "\n",
    "    States:\n",
    "      0 = non-excavated\n",
    "      1 = transient anomaly / under observation\n",
    "      2 = excavated (persistent)\n",
    "    \"\"\"\n",
    "    df = monitordf.copy()\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "    df = df.sort_values([\"y\", \"x\", \"time\"])\n",
    "\n",
    "\n",
    "    # ==== 1. Basic per-pixel NDVI time derivatives ====\n",
    "    # Keep NDVI NaNs; do NOT drop rows here, they carry cloud info.\n",
    "    df[\"NDVI_prev\"] = df.groupby([\"y\", \"x\"], group_keys=False)[\"NDVI\"].shift(1)\n",
    "    df[\"time_prev\"] = df.groupby([\"y\", \"x\"], group_keys=False)[\"time\"].shift(1)\n",
    "    df[\"delta_days\"] = (df[\"time\"] - df[\"time_prev\"]).dt.days\n",
    "\n",
    "\n",
    "    # Only compute deltaNDVI when both NDVI values exist and gap is not too large\n",
    "    valid_delta_mask = (\n",
    "        df[\"NDVI_prev\"].notna()\n",
    "        & df[\"NDVI\"].notna()\n",
    "        & df[\"delta_days\"].notna()\n",
    "        & (df[\"delta_days\"] <= max_gap_days_for_delta)\n",
    "    )\n",
    "    df[\"deltaNDVI\"] = np.nan\n",
    "    df.loc[valid_delta_mask, \"deltaNDVI\"] = (\n",
    "        df.loc[valid_delta_mask, \"NDVI\"] - df.loc[valid_delta_mask, \"NDVI_prev\"]\n",
    "    )\n",
    "\n",
    "\n",
    "    # Rolling slope (over valid NDVI values only)\n",
    "    def rolling_slope(series):\n",
    "        series = series.dropna()\n",
    "        x = np.arange(len(series))\n",
    "        if len(series) < 2:\n",
    "            return np.nan\n",
    "        return np.polyfit(x, series.values, 1)[0]\n",
    "\n",
    "\n",
    "    df[\"NDVI_slope\"] = (\n",
    "        df.groupby([\"y\", \"x\"], group_keys=False)[\"NDVI\"]\n",
    "        .rolling(window=window, min_periods=2)\n",
    "        .apply(rolling_slope, raw=False)\n",
    "        .reset_index(level=[0, 1], drop=True)\n",
    "    )\n",
    "\n",
    "\n",
    "    # ==== 2. Anomaly thresholds ====\n",
    "    # Anomaly threshold from monitoring distribution\n",
    "    ANOMALY_THRESH = df[\"anomalyscore\"].quantile(anomaly_quantile)\n",
    "\n",
    "\n",
    "    # Per-pixel NDVI drop threshold, with global fallback if series too short\n",
    "    # Compute per-pixel 0.10 quantile when there are enough valid deltas.\n",
    "    per_pixel_q10 = (\n",
    "        df.groupby([\"y\", \"x\"], group_keys=False)[\"deltaNDVI\"]\n",
    "        .quantile(0.10, interpolation=\"linear\")\n",
    "        .rename(\"delta_q10\")\n",
    "    )\n",
    "    df = df.join(per_pixel_q10, on=[\"y\", \"x\"])\n",
    "\n",
    "\n",
    "    global_q10 = df[\"deltaNDVI\"].quantile(0.10)  # typically negative\n",
    "    if pd.isna(global_q10):\n",
    "        global_q10 = -0.05  # safe fallback\n",
    "\n",
    "\n",
    "    # Use per-pixel q10 where available, else global\n",
    "    df[\"NDVI_DROP_THRESH\"] = df[\"delta_q10\"].fillna(global_q10)\n",
    "\n",
    "\n",
    "    # Median temporal delta used to derive persistence lengths in time-steps\n",
    "    valid_time_deltas = df[\"delta_days\"].dropna()\n",
    "    median_delta = valid_time_deltas.median() if not valid_time_deltas.empty else 10\n",
    "\n",
    "\n",
    "    K_RISE = max(min_persist_rise, int(round(persistence_days_rise / median_delta)))\n",
    "    K_FALL = max(min_persist_fall, int(round(persistence_days_fall / median_delta)))\n",
    "\n",
    "\n",
    "    # ==== 3. Initial state based on anomaly + NDVI drop ====\n",
    "    df[\"stateraw\"] = 0\n",
    "    transition_mask = (\n",
    "        (df[\"anomalyscore\"] >= ANOMALY_THRESH)\n",
    "        & (df[\"deltaNDVI\"] <= df[\"NDVI_DROP_THRESH\"])\n",
    "    )\n",
    "    df.loc[transition_mask, \"stateraw\"] = 1\n",
    "\n",
    "\n",
    "    # Explicit no-data flag: if NDVI is NaN, mark separately so it doesnâ€™t affect persistence\n",
    "    df[\"is_nodata\"] = df[\"NDVI\"].isna()\n",
    "\n",
    "\n",
    "    # ==== 4. Persistence with reversible logic ====\n",
    "    def apply_persistence(group):\n",
    "        \"\"\"\n",
    "        group: rows for one (y,x), already time-sorted.\n",
    "        Uses K_RISE to promote to 2 and K_FALL to demote 2 back toward 0.\n",
    "        \"\"\"\n",
    "        states_raw = group[\"stateraw\"].values\n",
    "        nodata = group[\"is_nodata\"].values\n",
    "\n",
    "\n",
    "        final = np.zeros(len(group), dtype=np.int8)\n",
    "        current_state = 0  # 0, 1, or 2\n",
    "        run_rise = 0\n",
    "        run_fall = 0\n",
    "\n",
    "\n",
    "        for i in range(len(group)):\n",
    "            if nodata[i]:\n",
    "                # Do not change current_state on nodata; just propagate\n",
    "                final[i] = current_state\n",
    "                continue\n",
    "\n",
    "\n",
    "            if states_raw[i] == 1:\n",
    "                # Rising anomaly\n",
    "                run_rise += 1\n",
    "                run_fall = 0\n",
    "                if current_state < 2 and run_rise >= K_RISE:\n",
    "                    current_state = 2\n",
    "                elif current_state == 0:\n",
    "                    # transient anomaly, not yet persistent\n",
    "                    current_state = 1\n",
    "            else:\n",
    "                # No anomaly this step\n",
    "                run_rise = 0\n",
    "                # If currently in 2, count clean steps to allow demotion\n",
    "                if current_state == 2:\n",
    "                    run_fall += 1\n",
    "                    if run_fall >= K_FALL:\n",
    "                        current_state = 1  # or 0 if you want full reset\n",
    "                elif current_state == 1:\n",
    "                    # If anomaly disappears, slowly decay to 0\n",
    "                    run_fall += 1\n",
    "                    if run_fall >= K_FALL:\n",
    "                        current_state = 0\n",
    "                else:\n",
    "                    run_fall = 0\n",
    "\n",
    "\n",
    "            final[i] = current_state\n",
    "\n",
    "\n",
    "        return pd.Series(final, index=group.index)\n",
    "\n",
    "\n",
    "    df[\"state\"] = (\n",
    "        df.sort_values(\"time\")\n",
    "        .groupby([\"y\", \"x\"], group_keys=False)\n",
    "        .apply(apply_persistence)\n",
    "    )\n",
    "\n",
    "\n",
    "    # Excavated mask is state == 2\n",
    "    df[\"excavated\"] = (df[\"state\"] == 2).astype(np.uint8)\n",
    "\n",
    "\n",
    "    # ==== 5. Build xarray state / excavation maps ====\n",
    "    statemaps = {}\n",
    "    excavationmaps = {}\n",
    "\n",
    "\n",
    "    for t, dft in df.groupby(\"time\"):\n",
    "        # Important: keep original y,x coordinates as in monitordf\n",
    "        statemaps[t] = (\n",
    "            dft.set_index([\"y\", \"x\"])[\"state\"]\n",
    "            .to_xarray()\n",
    "            .rename(\"state\")\n",
    "        )\n",
    "        excavationmaps[t] = (\n",
    "            dft.set_index([\"y\", \"x\"])[\"excavated\"]\n",
    "            .to_xarray()\n",
    "            .rename(\"excavated\")\n",
    "        )\n",
    "\n",
    "\n",
    "    return df, statemaps, excavationmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d09f0a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26805/3860329721.py:146: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(apply_persistence)\n"
     ]
    }
   ],
   "source": [
    "monitor_df = pd.read_csv(\"results/monitoring_anomaly_scores.csv\")\n",
    "\n",
    "monitor_df, state_maps, excavation_maps = run_excavation_pipeline(\n",
    "    monitor_df,\n",
    "    window=3,\n",
    "    anomaly_quantile=0.90,\n",
    "    #persistence_days=30\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75334fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
